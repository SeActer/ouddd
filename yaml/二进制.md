### k8s二进制搭建

##### 准备工作

```shell
#关闭防火墙
systemctl stop firewalld
systemctl disable firewalld
#关闭selinux
setenforce 0
sed -i 's/^SELINUX=.*/SELINUX=disabled/' /etc/selinux/config
#关闭swap
swapoff -a
sed -i '/ swap / s/^\(.*\)$/#\1/g' /etc/fstab
#开启 bridge-nf
modprobe br_netfilter
cat >> /etc/sysctl.conf << EOF
net.bridge.bridge-nf-call-iptables=1
net.bridge.bridge-nf-call-ip6tables=1
net.ipv4.ip_forward = 1
EOF
sysctl -p
#修改主机名
hostname node-x
cat  >>  /etc/hostname  << EOF
node-x
EOF
#添加host解析
cat >> /etc/hosts << EOF
x.x.x.x hostname1
y.y.y.y hostname2
EOF
mkdir /var/log/journal
mkdir /etc/systemd/journald.conf.d
cat >  /etc/systemd/journald.conf.d/prophet.conf <<EOF
[Journal]
#持久化保存到磁盘
Storage=persistent
# 压缩历史日志
Compress=yes
SyncIntervalSec=5m
RateLimitInterval=30s
RateLimitBurst=1000
# 最大占用空间
SystemMaxUse=5G
# 单个日志文件最大
SystemMaxFileSize=200M
#日志保存时间
MaxRetentionSec=2week
#不讲日志转发到syslog
ForwardToSyslog=no
EOF
systemctl restart systemd-journald
#配置chrony服务
yum install -y chrony ntpdate
cp -a /etc/chrony.conf /etc/chrony.conf.bak
#server端
cat > /etc/chrony.conf << EOF 
stratumweight 0 
driftfile /var/lib/chrony/drift 
rtcsync 
makestep 10 3 
allow 10.211.55.0/24  #允许同步的客户端地址
smoothtime 400 0.01

bindcmdaddress 127.0.0.1 
bindcmdaddress ::1
 
local stratum 8 
manual keyfile /etc/chrony.keys
#initstepslew 10 client1 client3 client6 
noclientlog 
logchange 0.5 
logdir /var/log/chrony 
EOF
systemctl restart chronyd.service 
systemctl enable  chronyd.service 
systemctl status chronyd.service
#client 
yum install -y chrony ntpdate
cp -a /etc/chrony.conf /etc/chrony.conf.bak
sed -i "s%^server%#server%g" /etc/chrony.conf 
echo "server 192.168.100.11 iburst" >> /etc/chrony.conf
ntpdate  192.168.100.11
systemctl restart chronyd.service 
systemctl enable  chronyd.service 
systemctl status chronyd.service

chronyc  sources #查看ntp_server状态
chronyc  tracking  #查看ntp详情信息
#加载ipvs内核模块
modprobe ip_vs
modprobe ip_vs_rr
modprobe ip_vs_wrr
modprobe ip_vs_sh
modprobe nf_conntrack_ipv4
```

##### 安装cfssl工具

```shell
github https://github.com/cloudﬂare/cfssl
官网 https://pkg.cfssl.org
curl -s -L -o  /usr/local/bin/cfssl https://pkg.cfssl.org/R1.2/cfssl_linux-amd64
curl -s -L -o  /usr/local/bin/cfssljson https://pkg.cfssl.org/R1.2/cfssljson_linux-amd64
curl -s -L -o  /usr/local/bin/cfssl-certinfo https://pkg.cfssl.org/R1.2/cfssl-certinfo_linux-amd64
chmod +x /usr/local/bin/cfssl*
```



##### 安装docker

```shell
wget https://download.docker.com/linux/static/stable/x86_64/docker-18.06.3-ce.tgz
tar xzvf docker-18.06.3-ce.tgz
cp docker/* /usr/lib/
cat >> /usr/lib/systemd/system/docker.service << EOF
[Unit]
Description=Docker Application Container Engine
Documentation=https://docs.docker.com
After=network.target firewalld.service

[Service]
Type=notify
# the default is not to use systemd for cgroups because the delegate issues still
# exists and systemd currently does not support the cgroup feature set required
# for containers run by docker
ExecStart=/usr/bin/dockerd
ExecReload=/bin/kill -s HUP $MAINPID
# Having non-zero Limit*s causes performance problems due to accounting overhead
# in the kernel. We recommend using cgroups to do container-local accounting.
LimitNOFILE=infinity
LimitNPROC=infinity
LimitCORE=infinity
# Uncomment TasksMax if your systemd version supports it.
# Only systemd 226 and above support this version.
#TasksMax=infinity
TimeoutStartSec=0
# set delegate yes so that systemd does not reset the cgroups of docker containers
Delegate=yes
# kill only the docker process, not all processes in the cgroup
KillMode=process

[Install]
WantedBy=multi-user.target
EOF

cat >> /etc/docker/daemon.json << EOF
{
"log-opts": {
"max-size": "100m"
},
"insecure-registries":[ "0.0.0.0/0"],
"storage-driver": "overlay2",
"graph": "/data",
"storage-opts": [
"overlay2.override_kernel_check=true"
  ]
}
EOF
systemctl daemon-reload
systemctl start docker
```

##### 生成根证书

```shell 
cat >> ca-config.json  << EOF
{
  "signing": {
    "default": {
      "expiry": "87600h"
    },
    "profiles": {
      "kubernetes": {
        "usages": [
            "signing",
            "key encipherment",
            "server auth",
            "client auth"
        ],
        "expiry": "87600h"
      }
    }
  }
}
EOF
cat >> ca-csr.json << EOF
{
  "CN": "kubernetes",
  "key": {
    "algo": "rsa",
    "size": 2048
  },
  "names": [
    {
      "C": "CN",
      "ST": "BeiJing",
      "L": "BeiJing",
      "O": "k8s",
      "OU": "System"
    }
  ]
}
EOF
# 生成证书和私钥
cfssl gencert -initca ca-csr.json | cfssljson -bare ca
# 生成完成后会有以下文件（我们最终想要的就是ca-key.pem和ca.pem，一个秘钥，一个证书）
$ ls
ca-config.json  ca.csr  ca-csr.json  ca-key.pem  ca.pem
```

##### ETCD集群v3.3.11

```shell
#证书
mkdir /opt/kubernetes/{bin,cfg,ssl}
cat >> etcd-csr.json << EOF
{
  "CN": "etcd",
  "hosts": [
    "127.0.0.1"
    "192.168.100.11",
    "192.168.100.12",
    "192.168.100.13"
  ],
  "key": {
    "algo": "rsa",
    "size": 2048
  },
  "names": [
    {
      "C": "CN",
      "ST": "BeiJing",
      "L": "BeiJing"
    }
  ]
}
EOF
cfssl gencert -ca=ca.pem -ca-key=ca-key.pem -config=ca-config.json -profile=kubernetes etcd-csr.json | cfssljson -bare etcd
#生成文件如下
etcd.csr  etcd-csr.json  etcd-key.pem  etcd.pem
#下载解压
wget https://github.com/coreos/etcd/releases/download/v3.3.11/etcd-v3.3.11-linux-amd64.tar.gz
mv etcd-v3.3.11-linux-amd64/etcd* /opt/kubernetes/bin/
mv ca.pem etcd-key.pem  etcd.pem /opt/kubernetes/ssl/

cat >> /opt/kubernetes/cfg/etcd.conf << EOF
#[Member]
#自定义etcd节点的名称，集群内唯一
ETCD_NAME="etcd-1"
#定义etcd数据存放目录
ETCD_DATA_DIR="/date/default.etcd"
#定义本机和成员之间通信的地址
ETCD_LISTEN_PEER_URLS="https://192.168.100.11:2380"
#定义对外通信的地址
ETCD_LISTEN_CLIENT_URLS="https://192.168.100.11:2379"

#[Clustering]
定义该节点成员对等URL地址，且会通告集群的其余成员节点
ETCD_INITIAL_ADVERTISE_PEER_URLS="https://192.168.100.11:2380"
#此成员的客户端URL列表，用于通告集群的其余部分
ETCD_ADVERTISE_CLIENT_URLS="https://192.168.100.11:2379"
#集群中所有节点的信息
ETCD_INITIAL_CLUSTER="etcd-1=https://192.168.100.11:2380,etcd-2=https://192.168.100.12:2380,etcd-3=https://192.168.100.13:2380"
#创建集群的token，这个值每个集群唯一
ETCD_INITIAL_CLUSTER_TOKEN="etcd-cluster"
#设置new为初始静态或DNS引导期间出现的所有成员。如果将此项设置为existing，则etcd将尝试加入现有集群
ETCD_INITIAL_CLUSTER_STATE="new"
EOF

cat >> /usr/lib/systemd/system/etcd.service  << EOF
[Unit]
Description=Etcd Server
After=network.target
After=network-online.target
Wants=network-online.target

[Service]
Type=notify
EnvironmentFile=/opt/kubernetes/cfg/etcd.conf
ExecStart=/opt/kubernetes/bin/etcd \
--name=${ETCD_NAME} \
--data-dir=${ETCD_DATA_DIR} \
--listen-peer-urls=${ETCD_LISTEN_PEER_URLS} \
--listen-client-urls=${ETCD_LISTEN_CLIENT_URLS},http://127.0.0.1:2379 \
--advertise-client-urls=${ETCD_ADVERTISE_CLIENT_URLS} \
--initial-advertise-peer-urls=${ETCD_INITIAL_ADVERTISE_PEER_URLS} \
--initial-cluster=${ETCD_INITIAL_CLUSTER} \
--initial-cluster-token=${ETCD_INITIAL_CLUSTER_TOKEN} \
--initial-cluster-state=new \
--cert-file=/opt/kubernetes/ssl/etcd.pem \
--key-file=/opt/kubernetes/ssl/etcd-key.pem \
--peer-cert-file=/opt/kubernetes/ssl/etcd.pem \
--peer-key-file=/opt/kubernetes/ssl/etcd-key.pem \
--trusted-ca-file=/opt/kubernetes/ssl/ca.pem \
--peer-trusted-ca-file=/opt/kubernetes/ssl/ca.pem
Restart=on-failure
LimitNOFILE=65536

[Install]
WantedBy=multi-user.target
EOF
#将/opt/kubernetes/ssl/ca.pem,/opt/kubernetes/ssl/etcd-key.pem,/opt/kubernetes/ssl/etcd.pem,/opt/kubernetes/cfg/etcd.conf,/usr/lib/systemd/system/etcd.service,/opt/kubernetes/bin/etcd*scp到192.168.100.12,192.168.100.13上，然后修改配配置文件中的地址
systemctl daemon-reload
systemctl enable etcd
systemctl start etcd
分别在三台上执行，第一台上执行可能会一直卡住，是在等待其他节点
systemctl status etcd
#检查
/opt/kubernetes/bin/etcdctl --ca-file=/opt/kubernetes/ssl/ca.pem --cert-file=/opt/kubernetes/ssl/etcd.pem --key-file=/opt/kubernetes/ssl/etcd-key.pem --endpoints="https://192.168.100.11:2379,etcd-2=https://192.168.100.12:2379,etcd-3=https://192.168.100.13:2379" cluster-health

#卸载etcd集群
systemctl stop etcd
systemctl disable etcd
rm -rf /opt/kubernetes/ssl/etcd*.pem
rm -rf /opt/kubernetes/bin/etcd*
rm -rf /opt/kubernetes/cfg/etcd.conf
rm -rf /usr/lib/systemd/system/etcd.service
rm -rf /date/default.etcd
```

##### 部署master

```shell
#在github上下载二进制软件包，v1.14.2版本
tar xzvf kubernetes-server-linux-amd64.tar.gz
cp kubernetes/server/bin/kube-apiserver kubernetes/server/bin/kube-controller-manager  kubernetes/server/bin/kube-scheduler /opt/kubernetes/bin/
cp kubernetes/server/bin/kubectl /usr/bin/
#证书
# hosts字段需要把所有的master节点，复制均衡节点的ip和vip，以及service-cluster-ip-range的第一个地址加进去，kubernetes*和127.0.0.1不要修改
# apiserver证书的json文件
cat >> apiserver-csr.json  << EOF
{
  "CN": "kubernetes",
  "hosts": [
    "10.0.0.1",  
    "127.0.0.1",
    "192.168.100.11",
    "192.168.100.12",
    "192.168.100.13",
    "192.168.100.100",
    "kubernetes",
    "kubernetes.default",
    "kubernetes.default.svc",
    "kubernetes.default.svc.cluster",
    "kubernetes.default.svc.cluster.local"
  ],
  "key": {
    "algo": "rsa",
    "size": 2048
  },
  "names": [
    {
      "C": "CN",
      "ST": "BeiJing",
      "L": "BeiJing",
      "O": "k8s",
      "OU": "System"
    }
  ]
}
EOF
# 为apiserver生成自签证书
cfssl gencert -ca=ca.pem -ca-key=ca-key.pem -config=ca-config.json -profile=kubernetes apiserver-csr.json | cfssljson -bare apiserver

cat >> /usr/lib/systemd/system/kube-apiserver.service <<EOF
[Unit]
Description=Kubernetes API Server
Documentation=https://github.com/kubernetes/kubernetes
[Service]
ExecStart=/opt/kubernetes/bin/kube-apiserver
--logtostderr=false \
--v=2 \
--log-dir=/opt/kubernetes/logs \
-- etcdservers=https://192.168.100.11:2379,https://192.168.100.12:2379,https://192.168.100.13:2379 \
--bind-address=192.168.100.11 \
--secure-port=6443 \
--advertise-address=192.168.100.11 \
--allow-privileged=true \
--service-cluster-ip-range=10.0.0.0/24 \
--enable-admission-plugins=NamespaceLifecycle,LimitRanger,ServiceAccount,ResourceQuota,NodeRestriction \
--authorization-mode=RBAC,Node \
--enable-bootstrap-token-auth=true \
--token-auth-file=/opt/kubernetes/cfg/token.csv \
--service-node-port-range=30000-32767 \
--kubelet-client-certificate=/opt/kubernetes/ssl/apiserver.pem \
--kubelet-client-key=/opt/kubernetes/ssl/apiserver-key.pem \
--tls-cert-file=/opt/kubernetes/ssl/apiserver.pem  \
--tls-private-key-file=/opt/kubernetes/ssl/apiserver-key.pem \
--client-ca-file=/opt/kubernetes/ssl/ca.pem \
--service-account-key-file=/opt/kubernetes/ssl/ca-key.pem \
--etcd-cafile=/opt/etcd/ssl/ca.pem \
--etcd-certfile=/opt/etcd/ssl/etcd.pem \
--etcd-keyfile=/opt/etcd/ssl/etcd-key.pem \
--audit-log-maxage=30 \
--audit-log-maxbackup=3 \
--audit-log-maxsize=100 \
--audit-log-path=/opt/kubernetes/logs/k8s-audit.log"
Restart=on-failure

[Install]
WantedBy=multi-user.target
eof

--logtostderr：日志是否输出到标准错误输出
--v=2：日志级别0-8，数字越大，日志越详细
--log-dir：设置日志存放目录
--etcd-servers：指定etcd服务的URL
--bind-address：apiserver监听地址
--secure-port：apiserver监听端口，默认为6443
--advertise-address：通告地址，让其他节点通过此IP来连接apiserver
--allow-privileged：开启容器的privileged权限
--service-cluster-ip-range：Kubernetes集群中Service的虚拟IP地址范围，以CIDR格式表示，例如169.169.0.0/16，该IP范围不能与部署机器的IP地址有重合
--enable-admission-plugins：Kubernetes集群的准⼊控制设置，各控制模块以插件的形式依次生效
--authorization-mode：授权模式
--enable-bootstrap-token-auth：启用bootstrap token认证
--service-node-port-range：Kubernetes集群中Service可使用的端口号范围，默认值为30000-32767（包含两端
--kubelet-client-certificate、--kubelet-client-key：连接kubelet使用的证书和私钥
--tls-cert-file、--tls-private-key-file、--client-ca-file、--service-account-key-file：apiserver启用https所⽤的证书和私钥
--etcd-cafile、--etcd-certfile、--etcd-keyfile：连接etcd所使用的证书
--audit-log-maxage、--audit-log-maxbackup、--audit-log-maxsize、--audit-log-path：日志轮转、日志路径
#三台主机上部署api-server，然后
cat >> /opt/kubernetes/cfg/token.csv << EOF
017720f4995c60f3347b8ddd178b2816,kubelet-bootstrap,10001,'system:kubelet-bootstrap'
EOF

#将service文件，svc文件，pem文件，传到另外两个机器上，修好ip，
systemctl daemon-reload
systemctl enable kube-apiserver
systemctl restart kube-apiserver
journalctl -f -u kube-apiserver
systemctl status kube-apiserver
netstat -ntlp
```

```shell
#部署keepalived，实现apiserver的高可用
yum install -y keepailved
#主keepalived，
cat >> keepalived-master.conf  <<EOF
! Configuration File for keepalived
global_defs {
 router_id keepalive-master
}

vrrp_script check_apiserver {
 script "/etc/keepalived/check-apiserver.sh"
 interval 3
 weight -2
}

vrrp_instance VI-kube-master {
   state MASTER
   interface eth0
   virtual_router_id 68
   priority 100
   dont_track_primary
   advert_int 3
   virtual_ipaddress {
     192.168.100.100
   }
   track_script {
       check_apiserver
   }
}
EOF
cat >> check-apiserver.sh << EOF
#!/bin/sh

errorExit() {
   echo "*** $*" 1>&2
   exit 1
}

curl --silent --max-time 2 --insecure https://127.0.0.1:6443/ -o /dev/null || errorExit "Error GET https://127.0.0.1:6443/"
if ip addr | grep -q 172.18.41.14; then
   curl --silent --max-time 2 --insecure https://192.168.100.100:6443/ -o /dev/null || errorExit "Error GET https://192.168.100.100:6443/"
fi
EOF
#备机
[root@node11 configs]# cat keepalived-backup.conf 
! Configuration File for keepalived
global_defs {
 router_id keepalive-backup
}

vrrp_script check_apiserver {
 script "/etc/keepalived/check-apiserver.sh"
 interval 3
 weight -2
}

vrrp_instance VI-kube-master {
   state BACKUP
   interface eth0
   virtual_router_id 68
   priority 99
   dont_track_primary
   advert_int 3
   virtual_ipaddress {
     192.168.100.100
   }
   track_script {
       check_apiserver
   }
}
EOF
cat >> check-apiserver.sh << EOF
#!/bin/sh

errorExit() {
   echo "*** $*" 1>&2
   exit 1
}

curl --silent --max-time 2 --insecure https://127.0.0.1:6443/ -o /dev/null || errorExit "Error GET https://127.0.0.1:6443/"
if ip addr | grep -q 172.18.41.14; then
   curl --silent --max-time 2 --insecure https://192.168.100.100:6443/ -o /dev/null || errorExit "Error GET https://192.168.100.100:6443/"
fi
EOF

# 分别在master和backup上启动服务
systemctl enable keepalived
systemctl start keepalived

# 检查状态
systemctl status keepalived

# 查看日志
journalctl -f -u keepalived

# 访问测试
curl --insecure https://<master-vip>:6443/
```



```shell
#部署kubectl（管理端节点）
#kubectl 与 apiserver https 安全端口通信，apiserver 对提供的证书进行认证和授权。 kubectl 作为集群的管理工具，需要被授予最高权限。这里创建具有最高权限的 admin 证书
cat >> admin-csr.json << EOF
{
  "CN": "admin",
  "hosts": [],
  "key": {
    "algo": "rsa",
    "size": 2048
  },
  "names": [
    {
      "C": "CN",
      "ST": "BeiJing",
      "L": "BeiJing",
      "O": "system:masters",
      "OU": "System"
    }
  ]
}
EOF
cfssl gencert -ca=ca.pem -ca-key=ca-key.pem -config=ca-config.json -profile=kubernetes admin-csr.json | cfssljson -bare admin
#创建kubeconfig配置文件
#kubeconfig 为 kubectl 的配置文件，包含访问 apiserver 的所有信息，如 apiserver 地址、CA 证书和自身使用的证书
# 设置集群参数
$ kubectl config set-cluster kubernetes \
  --certificate-authority=ca.pem \
  --embed-certs=true \
  --server=https://192.168.100.100:6443 \
  --kubeconfig=kube.config

# 设置客户端认证参数
$ kubectl config set-credentials admin \
  --client-certificate=admin.pem \
  --client-key=admin-key.pem \
  --embed-certs=true \
  --kubeconfig=kube.config

# 设置上下文参数
$ kubectl config set-context kubernetes \
  --cluster=kubernetes \
  --user=admin \
  --kubeconfig=kube.config
  
# 设置默认上下文
$ kubectl config use-context kubernetes --kubeconfig=kube.config

#移动到目标位置
$ mv kube.config  ~/.kube/config

#授予 kubernetes 证书访问 kubelet API 的权限
#在执行 kubectl exec、run、logs 等命令时，apiserver 会转发到 kubelet。这里定义 RBAC 规则，授权 apiserver 调用 kubelet API。
kubectl create clusterrolebinding kube-apiserver:kubelet-apis --clusterrole=system:kubelet-api-admin --user kubernetes
#小测试
#查看集群信息
$ kubectl cluster-info
$ kubectl get all --all-namespaces
$ kubectl get componentstatuses
```



```shell
#部署controller-manager
# controller-manager启动后将通过竞争选举机制产生一个 leader 节点，其它节点为阻塞状态。当 leader 节点不可用后，剩余节点将再次进行选举产生新的 leader 节点，从而保证服务的可用性。
# 创建证书和私钥
cat  >> controller-manager-csr.json  << EOF
{
    "CN": "system:kube-controller-manager",
    "key": {
        "algo": "rsa",
        "size": 2048
    },
    "hosts": [
    "127.0.0.1",
    "192.168.100.11",
    "192.168.100.12",
    "192.168.100.13"
    ],
    "names": [
      {
        "C": "CN",
        "ST": "BeiJing",
        "L": "BeiJing",
        "O": "system:kube-controller-manager",
        "OU": "System"
      }
    ]
}
EOF
#为controller-manager生成证书
cfssl gencert -ca=ca.pem -ca-key=ca-key.pem  -config=ca-config.json  -profile=kubernetes controller-manager-csr.json | cfssljson -bare controller-manager

#创建controller-manager的kubeconfig
$ kubectl config set-cluster kubernetes \
  --certificate-authority=ca.pem \
  --embed-certs=true \
  --server=https://192.168.100.100:6443 \
  --kubeconfig=controller-manager.kubeconfig

$ kubectl config set-credentials system:kube-controller-manager \
  --client-certificate=controller-manager.pem \
  --client-key=controller-manager-key.pem \
  --embed-certs=true \
  --kubeconfig=controller-manager.kubeconfig

$ kubectl config set-context system:kube-controller-manager \
  --cluster=kubernetes \
  --user=system:kube-controller-manager \
  --kubeconfig=controller-manager.kubeconfig

$ kubectl config use-context system:kube-controller-manager --kubeconfig=controller-manager.kubeconfig

# 分发controller-manager.kubeconfig
$ scp controller-manager.kubeconfig <user>@<node-ip>:/opt/kubernetes/cfg

#service文件
cat >> /usr/lib/systemd/system/kube-controller-manager.service << EOF
[Unit]
Description=Kubernetes Controller Manager
Documentation=https://github.com/GoogleCloudPlatform/kubernetes

[Service]
ExecStart=/opt/kubernetes/bin/kube-controller-manager \
--logtostderr=false \
--leader-elect=true \
--address=0.0.0.0 \
--service-cluster-ip-range=10.0.0.0/24 \
--cluster-cidr=10.244.0.0/16 \
--node-cidr-mask-size=24 \
--cluster-name=kubernetes \
--allocate-node-cidrs=true \
--kubeconfig=/opt/kubernetes/cfg/controller-manager.kubeconfig \
--authentication-kubeconfig=/opt/kubernetes/cfg/controller-manager.kubeconfig \
--authorization-kubeconfig=/opt/kubernetes/cfg/controller-manager.kubeconfig \
--use-service-account-credentials=true \
--client-ca-file=/opt/kubernetes/ssl/ca.pem \
--requestheader-client-ca-file=/opt/kubernetes/ssl/ca.pem \
--node-monitor-grace-period=40s \
--node-monitor-period=5s \
--pod-eviction-timeout=5m0s \
--terminated-pod-gc-threshold=50 \
--alsologtostderr=true \
--cluster-signing-cert-file=/opt/kubernetes/ssl/ca.pem \
--cluster-signing-key-file=/opt/kubernetes/ssl/ca-key.pem  \
--deployment-controller-sync-period=10s \
--experimental-cluster-signing-duration=86700h0m0s \
--enable-garbage-collector=true \
--root-ca-file=/opt/kubernetes/ssl/k8s/ca.pem \
--service-account-private-key-file=/opt/kubernetes/ssl/k8s/ca-key.pem \
--feature-gates=RotateKubeletServerCertificate=true,RotateKubeletClientCertificate=true \
--controllers=*,bootstrapsigner,tokencleaner \
--horizontal-pod-autoscaler-use-rest-clients=true \
--horizontal-pod-autoscaler-sync-period=10s \
--tls-cert-file=/opt/kubernetes/ssl/controller-manager.pem \
--tls-private-key-file=/opt/kubernetes/ssl/controller-manager-key.pem \
--kube-api-qps=100 \
--kube-api-burst=100 \
--log-dir=/opt/kubernetes/log \
--v=2
Restart=on-failure
RestartSec=5

[Install]
WantedBy=multi-user.target
EOF

#将controller-manager-key.pem，controller-manager.pem，controller-manager.kubeconfig，kube-controller-manager.service等文件拷贝到另外两个master节点

systemctl daemon-reload
systemctl enable kube-controller-manager
systemctl restart kube-controller-manager
systemctl status kube-controller-manager
journalctl -f -u kube-controller-manager
# 查看leader
kubectl get endpoints kube-controller-manager --namespace=kube-system -o yaml

```



```shell
#scheduler
scheduler启动后将通过竞争选举机制产生一个 leader 节点，其它节点为阻塞状态。当 leader 节点不可用后，剩余节点将再次进行选举产生新的 leader 节点，从而保证服务的可用性。
cat >> scheduler-csr.json  << EOF
{
    "CN": "system:kube-scheduler",
    "hosts": [
      "127.0.0.1",
    "192.168.100.11",
    "192.168.100.12",
    "192.168.100.13"
    ],
    "key": {
        "algo": "rsa",
        "size": 2048
    },
    "names": [
      {
        "C": "CN",
        "ST": "BeiJing",
        "L": "BeiJing",
        "O": "system:kube-scheduler",
        "OU": "System"
      }
    ]
}
EOF
#为scheduler生成证书
cfssl gencert -ca=../ca.pem -ca-key=../ca-key.pem -config=../ca-config.json -profile=kubernetes scheduler-csr.json | cfssljson -bare kube-scheduler

# 创建scheduler的kubeconfig
kubectl config set-cluster kubernetes \
  --certificate-authority=ca.pem \
  --embed-certs=true \
  --server=https://192.168.100.100:6443 \
  --kubeconfig=kube-scheduler.kubeconfig

kubectl config set-credentials system:kube-scheduler \
  --client-certificate=kube-scheduler.pem \
  --client-key=kube-scheduler-key.pem \
  --embed-certs=true \
  --kubeconfig=kube-scheduler.kubeconfig

kubectl config set-context system:kube-scheduler \
  --cluster=kubernetes \
  --user=system:kube-scheduler \
  --kubeconfig=kube-scheduler.kubeconfig

kubectl config use-context system:kube-scheduler --kubeconfig=kube-scheduler.kubeconfig

#创建service文件
cat >> /usr/lib/systemd/system/kube-scheduler.service << EOF
[Unit]
Description=Kubernetes Scheduler
Documentation=https://github.com/GoogleCloudPlatform/kubernetes

[Service]
ExecStart=/opt/kubernetes/bin/kube-scheduler \
  --address=127.0.0.1 \
  --kubeconfig=/opt/kubernetes/cfg/kube-scheduler.kubeconfig \
  --leader-elect=true \
  --alsologtostderr=true \
  --logtostderr=false \
  --log-dir=/opt/kubernetes/log/ \
  --v=2
Restart=on-failure
RestartSec=5

[Install]
WantedBy=multi-user.target
EOF
#移动对应的文件到其他主机上
systemctl daemon-reload
systemctl enable kube-scheduler
systemctl restart kube-scheduler
journalctl -f -u kube-scheduler
systemctl status kube-scheduler
#查看leader
kubectl get endpoints kube-scheduler --namespace=kube-system -o yaml
```



```shell
#部署kubelet
cat >> /opt/kubernetes/cfg/bootstrap.kubeconfig << EOF
apiVersion: v1
clusters:
- cluster:
    certificate-authority: /opt/kubernetes/ssl/ca.pem
    server: https://192.168.100.100:6443
  name: kubernetes
contexts:
- context:
    cluster: kubernetes
    user: kubelet-bootstrap
  name: default
current-context: default
kind: Config
preferences: {}
users:
- name: kubelet-bootstrap
  user:
    token: 017720f4995c60f3347b8ddd178b2816   
EO
#token内容为apiserver指向的那个csv文件的前面那个

cat >> /opt/kubernetes/cfg/kubelet-config.yml << EOF
kind: KubeletConfiguration
apiVersion: kubelet.config.k8s.io/v1beta1
address: 0.0.0.0
port: 10250
readOnlyPort: 10255
cgroupDriver: cgroupfs
clusterDNS:
- 10.0.0.2
clusterDomain: cluster.local
failSwapOn: false
authentication:
  anonymous:
    enabled: false
  webhook:
    cacheTTL: 2m0s
    enabled: true
  x509:
    clientCAFile: /opt/kubernetes/ssl/ca.pem
authorization:
  mode: Webhook
  webhook:
    cacheAuthorizedTTL: 5m0s
    cacheUnauthorizedTTL: 30s
evictionHard:
  imagefs.available: 15%
  memory.available: 100Mi
  nodefs.available: 10%
  nodefs.inodesFree: 5%
maxOpenFiles: 1000000
maxPods: 110
EOF
cat >> /usr/lib/systemd/system/kubelet.service << EOF
[Unit]
Description=Kubernetes Kubelet
Documentation=https://github.com/GoogleCloudPlatform/kubernetes
After=docker.service
Requires=docker.service

[Service]
WorkingDirectory=/var/lib/kubelet
ExecStart=/opt/kubernetes/bin/kubelet \
  --bootstrap-kubeconfig=/opt/kubernetes/cfg/bootstrap.kubeconfig \
  --cert-dir=/opt/kubernetes/ssl \
  --kubeconfig=/opt/kubernetes/cfg/kubelet.kubeconfig \
  --config=/opt/kubernetes/cfg/kubelet-config.yml \
  --network-plugin=cni \
  --pod-infra-container-image=registry.cn-hangzhou.aliyuncs.com/oyd/pause:3.1 \
  --alsologtostderr=true \
  --logtostderr=false \
  --log-dir=/opt/kubernetes/logs \
  --v=2
Restart=on-failure
RestartSec=5

[Install]
WantedBy=multi-user.target
EOF

#kublet 启动时查找配置的 --kubeletconfig 文件是否存在，如果不存在则使用 --bootstrap-kubeconfig 向 kube-apiserver 发送证书签名请求 (CSR)。
# bootstrap附权
kubectl create clusterrolebinding kubelet-bootstrap --clusterrole=system:node-bootstrapper --user=kubelet-bootstrap

# 启动服务
mkdir -p /var/lib/kubelet
systemctl daemon-reload && systemctl enable kubelet && systemctl restart kubelet

# 在master上Approve bootstrap请求
kubectl get csr
kubectl certificate approve <name> 

# 查看服务状态
service kubelet status

# 查看日志
journalctl -f -u kubelet
```



```shell
#创建证书和私钥
cat > kube-proxy-csr.json << EOF 
{
  "CN": "system:kube-proxy",
  "key": {
    "algo": "rsa",
    "size": 2048
  },
  "names": [
    {
      "C": "CN",
      "ST": "BeiJing",
      "L": "BeiJing",
      "O": "k8s",
      "OU": "System"
    }
  ]
}
EOF
#为kube-proxy生成证书
cfssl gencert -ca=ca.pem -ca-key=ca-key.pem -config=ca-config.json -profile=kubernetes kube-proxy-csr.json | cfssljson -bare kube-proxy

cat >> /opt/kubernetes/cfg/kube-proxy.kubeconfig << EOF
apiVersion: v1
clusters:
- cluster:
    certificate-authority: /opt/kubernetes/ssl/ca.pem
    server: https://192.168.100.100:6443
  name: kubernetes
contexts:
- context:
    cluster: kubernetes
    user: kube-proxy
  name: default
current-context: default
kind: Config
preferences: {}
users:
- name: kube-proxy
  user:
    client-certificate: /opt/kubernetes/ssl/kube-proxy.pem
    client-key: /opt/kubernetes/ssl/kube-proxy-key.pem
EOF

cat >> /opt/kubernetes/cfg/kube-proxy-config.yml << EOF
kind: KubeProxyConfiguration
apiVersion: kubeproxy.config.k8s.io/v1alpha1
address: 0.0.0.0
metricsBindAddress: 0.0.0.0:10249
clientConnection:
  kubeconfig: /opt/kubernetes/cfg/kube-proxy.kubeconfig
hostnameOverride: k8s-node1
clusterCIDR: 10.0.0.0/24
mode: ipvs
ipvs:
  scheduler: "rr"
iptables:  
  masqueradeAll: true
EOF

cat >> /usr/lib/systemd/system/kube-proxy.service << EOF
[Unit]
Description=Kubernetes Kube-Proxy Server
Documentation=https://github.com/GoogleCloudPlatform/kubernetes
After=network.target

[Service]
WorkingDirectory=/var/lib/kube-proxy
ExecStart=/opt/kubernetes/bin/kube-proxy \
  --config=/opt/kubernetes/cfg/kube-proxy.config.yaml \
  --alsologtostderr=true \
  --logtostderr=false \
  --log-dir=/opt/kubernetes/log/ \
  --v=2
Restart=on-failure
RestartSec=5
LimitNOFILE=65536

[Install]
WantedBy=multi-user.target
EOF

# 创建依赖目录
mkdir -p /var/lib/kube-proxy 

# 启动服务
systemctl daemon-reload && systemctl enable kube-proxy && systemctl restart kube-proxy

# 查看状态
systemctl status kube-proxy

# 查看日志
journalctl -f -u kube-proxy
```












